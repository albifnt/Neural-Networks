{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network Model with Two Layers\n",
    "\n",
    "## Neural Network Model with Two Layers for a Single Training Example\n",
    "\n",
    "The input and output layers of the neural network are the same as for one perceptron model, but there is a **hidden layer** now in between them. The training examples $x^{(i)}=\\begin{bmatrix}x_1^{(i)} \\\\ x_2^{(i)}\\end{bmatrix}$ from the input layer of size $n_x = 2$ are first fed into the hidden layer of size $n_h = 2$. They are simultaneously fed into the first perceptron with weights $W_1^{[1]}=\\begin{bmatrix}w_{1,1}^{[1]} & w_{2,1}^{[1]}\\end{bmatrix}$, bias  $b_1^{[1]}$; and into the second perceptron with weights $W_2^{[1]}=\\begin{bmatrix}w_{1,2}^{[1]} & w_{2,2}^{[1]}\\end{bmatrix}$, bias $b_2^{[1]}$. The integer in the square brackets $^{[1]}$ denotes the layer number, because there are two layers now with their own parameters and outputs, which need to be distinguished. \n",
    "\n",
    "\\begin{align}\n",
    "z_1^{[1](i)} &= w_{1,1}^{[1]} x_1^{(i)} + w_{2,1}^{[1]} x_2^{(i)} + b_1^{[1]} = W_1^{[1]}x^{(i)} + b_1^{[1]},\\\\\n",
    "z_2^{[1](i)} &= w_{1,2}^{[1]} x_1^{(i)} + w_{2,2}^{[1]} x_2^{(i)} + b_2^{[1]} = W_2^{[1]}x^{(i)} + b_2^{[1]}.\\tag{1}\n",
    "\\end{align}\n",
    "\n",
    "These expressions for one training example $x^{(i)}$ can be rewritten in a matrix form :\n",
    "\n",
    "$$z^{[1](i)} = W^{[1]} x^{(i)} + b^{[1]},\\tag{2}$$\n",
    "\n",
    "where \n",
    "\n",
    "&emsp; &emsp; $z^{[1](i)} = \\begin{bmatrix}z_1^{[1](i)} \\\\ z_2^{[1](i)}\\end{bmatrix}$ is vector of size $\\left(n_h \\times 1\\right) = \\left(2 \\times 1\\right)$; \n",
    "\n",
    "&emsp; &emsp; $W^{[1]} = \\begin{bmatrix}W_1^{[1]} \\\\ W_2^{[1]}\\end{bmatrix} = \n",
    "\\begin{bmatrix}w_{1,1}^{[1]} & w_{2,1}^{[1]} \\\\ w_{1,2}^{[1]} & w_{2,2}^{[1]}\\end{bmatrix}$ is matrix of size $\\left(n_h \\times n_x\\right) = \\left(2 \\times 2\\right)$;\n",
    "\n",
    "&emsp; &emsp; $b^{[1]} = \\begin{bmatrix}b_1^{[1]} \\\\ b_2^{[1]}\\end{bmatrix}$ is vector of size $\\left(n_h \\times 1\\right) = \\left(2 \\times 1\\right)$.\n",
    "\n",
    "Next, the hidden layer activation function needs to be applied for each of the elements in the vector $z^{[1](i)}$. Various activation functions can be used here and in this model you will take the sigmoid function $\\sigma\\left(x\\right) = \\frac{1}{1 + e^{-x}}$. Remember that its derivative is $\\frac{d\\sigma}{dx} = \\sigma\\left(x\\right)\\left(1-\\sigma\\left(x\\right)\\right)$. The output of the hidden layer is a vector of size $\\left(n_h \\times 1\\right) = \\left(2 \\times 1\\right)$:\n",
    "\n",
    "$$a^{[1](i)} = \\sigma\\left(z^{[1](i)}\\right) = \n",
    "\\begin{bmatrix}\\sigma\\left(z_1^{[1](i)}\\right) \\\\ \\sigma\\left(z_2^{[1](i)}\\right)\\end{bmatrix}.\\tag{3}$$\n",
    "\n",
    "Then the hidden layer output gets fed into the output layer of size $n_y = 1$. This was covered in the previous lab, the only difference are: $a^{[1](i)}$ is taken instead of $x^{(i)}$ and layer notation $^{[2]}$ appears to identify all parameters and outputs:\n",
    "\n",
    "$$z^{[2](i)} = w_1^{[2]} a_1^{[1](i)} + w_2^{[2]} a_2^{[1](i)} + b^{[2]}= W^{[2]} a^{[1](i)} + b^{[2]},\\tag{4}$$\n",
    "\n",
    "&emsp; &emsp; $z^{[2](i)}$ and $b^{[2]}$ are scalars for this model, as $\\left(n_y \\times 1\\right) = \\left(1 \\times 1\\right)$; \n",
    "\n",
    "&emsp; &emsp; $W^{[2]} = \\begin{bmatrix}w_1^{[2]} & w_2^{[2]}\\end{bmatrix}$ is vector of size $\\left(n_y \\times n_h\\right) = \\left(1 \\times 2\\right)$.\n",
    "\n",
    "Finally, the same sigmoid function is used as the output layer activation function:\n",
    "\n",
    "$$a^{[2](i)} = \\sigma\\left(z^{[2](i)}\\right).\\tag{5}$$\n",
    "\n",
    "Mathematically the two layer neural network model for each training example $x^{(i)}$ can be written with the expressions $(2) - (5)$. Let's rewrite them next to each other for convenience:\n",
    "\n",
    "\\begin{align}\n",
    "z^{[1](i)} &= W^{[1]} x^{(i)} + b^{[1]},\\\\\n",
    "a^{[1](i)} &= \\sigma\\left(z^{[1](i)}\\right),\\\\\n",
    "z^{[2](i)} &= W^{[2]} a^{[1](i)} + b^{[2]},\\\\\n",
    "a^{[2](i)} &= \\sigma\\left(z^{[2](i)}\\right).\\\\\n",
    "\\tag{6}\n",
    "\\end{align}\n",
    "\n",
    "Note, that all of the parameters to be trained in the model are without $^{(i)}$ index - they are independent on the input data.\n",
    "\n",
    "Finally, the predictions for some example $x^{(i)}$ can be made taking the output $a^{[2](i)}$ and calculating $\\hat{y}$ as: \n",
    "if $a^{[2](i)}$ > 0 $\\hat{y}$ = 1 else $\\hat{y}$ = 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network Model with Two Layers for Multiple Training Examples\n",
    "\n",
    "Similarly to the single perceptron model, $m$ training examples can be organised in a matrix $X$ of a shape ($2 \\times m$), putting $x^{(i)}$ into columns. Then the model $(6)$ can be rewritten in terms of matrix multiplications:\n",
    "\n",
    "\\begin{align}\n",
    "Z^{[1]} &= W^{[1]} X + b^{[1]},\\\\\n",
    "A^{[1]} &= \\sigma\\left(Z^{[1]}\\right),\\\\\n",
    "Z^{[2]} &= W^{[2]} A^{[1]} + b^{[2]},\\\\\n",
    "A^{[2]} &= \\sigma\\left(Z^{[2]}\\right),\\\\\n",
    "\\tag{7}\n",
    "\\end{align}\n",
    "\n",
    "where $b^{[1]}$ is broadcasted to the matrix of size $\\left(n_h \\times m\\right) = \\left(2 \\times m\\right)$ and $b^{[2]}$ to the vector of size $\\left(n_y \\times m\\right) = \\left(1 \\times m\\right)$. It would be a good exercise for you to have a look at the expressions $(7)$ and check that sizes of the matrices will actually match to perform required multiplications.\n",
    "\n",
    "You have derived expressions to perform forward propagation. Time to evaluate your model and train it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cost Function and Training\n",
    "\n",
    "For the evaluation of this simple neural network you can use the same cost function as for the single perceptron case - log loss function. Originally initialized weights were just some random values, now you need to perform training of the model: find such set of parameters $W^{[1]}$, $b^{[1]}$, $W^{[2]}$, $b^{[2]}$, that will minimize the cost function.\n",
    "\n",
    "Like in the previous example of a single perceptron neural network, the cost function can be written as:\n",
    "\n",
    "$$\\mathcal{L}\\left(W^{[1]}, b^{[1]}, W^{[2]}, b^{[2]}\\right) = \\frac{1}{m}\\sum_{i=1}^{m} L\\left(W^{[1]}, b^{[1]}, W^{[2]}, b^{[2]}\\right) =  \\frac{1}{m}\\sum_{i=1}^{m}  \\large\\left(\\small - y^{(i)}\\log\\left(a^{[2](i)}\\right) - (1-y^{(i)})\\log\\left(1- a^{[2](i)}\\right)  \\large  \\right), \\small\\tag{8}$$\n",
    "\n",
    "where $y^{(i)} \\in \\{0,1\\}$ are the original labels and $a^{[2](i)}$ are the continuous output values of the forward propagation step (elements of array $A^{[2]}$).\n",
    "\n",
    "To minimize it, you can use gradient descent, updating the parameters with the following expressions:\n",
    "\n",
    "\\begin{align}\n",
    "W^{[1]} &= W^{[1]} - \\alpha \\frac{\\partial \\mathcal{L} }{ \\partial W^{[1]} },\\\\\n",
    "b^{[1]} &= b^{[1]} - \\alpha \\frac{\\partial \\mathcal{L} }{ \\partial b^{[1]} },\\\\\n",
    "W^{[2]} &= W^{[2]} - \\alpha \\frac{\\partial \\mathcal{L} }{ \\partial W^{[2]} },\\\\\n",
    "b^{[2]} &= b^{[2]} - \\alpha \\frac{\\partial \\mathcal{L} }{ \\partial b^{[2]} },\\\\\n",
    "\\tag{9}\n",
    "\\end{align}\n",
    "\n",
    "where $\\alpha$ is the learning rate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "To perform training of the model you need to calculate now $\\frac{\\partial \\mathcal{L} }{ \\partial W^{[1]}}$, $\\frac{\\partial \\mathcal{L} }{ \\partial b^{[1]}}$, $\\frac{\\partial \\mathcal{L} }{ \\partial W^{[2]}}$, $\\frac{\\partial \\mathcal{L} }{ \\partial b^{[2]}}$. \n",
    "\n",
    "Let's start from the end of the neural network. You can rewrite here the corresponding expressions for $\\frac{\\partial \\mathcal{L} }{ \\partial W }$ and $\\frac{\\partial \\mathcal{L} }{ \\partial b }$ from the single perceptron neural network:\n",
    "\n",
    "\\begin{align}\n",
    "\\frac{\\partial \\mathcal{L} }{ \\partial W } &= \n",
    "\\frac{1}{m}\\left(A-Y\\right)X^T,\\\\\n",
    "\\frac{\\partial \\mathcal{L} }{ \\partial b } &= \n",
    "\\frac{1}{m}\\left(A-Y\\right)\\mathbf{1},\\\\\n",
    "\\end{align}\n",
    "\n",
    "where $\\mathbf{1}$ is just a ($m \\times 1$) vector of ones. Your one perceptron is in the second layer now, so $W$ will be exchanged with $W^{[2]}$, $b$ with $b^{[2]}$, $A$ with $A^{[2]}$, $X$ with $A^{[1]}$:\n",
    "\n",
    "\\begin{align}\n",
    "\\frac{\\partial \\mathcal{L} }{ \\partial W^{[2]} } &= \n",
    "\\frac{1}{m}\\left(A^{[2]}-Y\\right)\\left(A^{[1]}\\right)^T,\\\\\n",
    "\\frac{\\partial \\mathcal{L} }{ \\partial b^{[2]} } &= \n",
    "\\frac{1}{m}\\left(A^{[2]}-Y\\right)\\mathbf{1}.\\\\\n",
    "\\tag{10}\n",
    "\\end{align}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now find $\\frac{\\partial \\mathcal{L} }{ \\partial W^{[1]}} = \n",
    "\\begin{bmatrix}\n",
    "\\frac{\\partial \\mathcal{L} }{ \\partial w_{1,1}^{[1]}} & \\frac{\\partial \\mathcal{L} }{ \\partial w_{2,1}^{[1]}} \\\\\n",
    "\\frac{\\partial \\mathcal{L} }{ \\partial w_{1,2}^{[1]}} & \\frac{\\partial \\mathcal{L} }{ \\partial w_{2,2}^{[1]}} \\end{bmatrix}$. It was shown in the videos that $$\\frac{\\partial \\mathcal{L} }{ \\partial w_{1,1}^{[1]}}=\\frac{1}{m}\\sum_{i=1}^{m} \\left( \n",
    "\\left(a^{[2](i)} - y^{(i)}\\right) \n",
    "w_1^{[2]} \n",
    "\\left(a_1^{[1](i)}\\left(1-a_1^{[1](i)}\\right)\\right)\n",
    "x_1^{(i)}\\right)\\tag{11}$$\n",
    "\n",
    "If you do this accurately for each of the elements $\\frac{\\partial \\mathcal{L} }{ \\partial W^{[1]}}$, you will get the following matrix:\n",
    "\n",
    "$$\\frac{\\partial \\mathcal{L} }{ \\partial W^{[1]}} = \\begin{bmatrix}\n",
    "\\frac{\\partial \\mathcal{L} }{ \\partial w_{1,1}^{[1]}} & \\frac{\\partial \\mathcal{L} }{ \\partial w_{2,1}^{[1]}} \\\\\n",
    "\\frac{\\partial \\mathcal{L} }{ \\partial w_{1,2}^{[1]}} & \\frac{\\partial \\mathcal{L} }{ \\partial w_{2,2}^{[1]}} \\end{bmatrix}$$\n",
    "$$= \\frac{1}{m}\\begin{bmatrix}\n",
    "\\sum_{i=1}^{m} \\left( \\left(a^{[2](i)} - y^{(i)}\\right) w_1^{[2]} \\left(a_1^{[1](i)}\\left(1-a_1^{[1](i)}\\right)\\right)\n",
    "x_1^{(i)}\\right) & \n",
    "\\sum_{i=1}^{m} \\left( \\left(a^{[2](i)} - y^{(i)}\\right) w_1^{[2]} \\left(a_1^{[1](i)}\\left(1-a_1^{[1](i)}\\right)\\right)\n",
    "x_2^{(i)}\\right)  \\\\\n",
    "\\sum_{i=1}^{m} \\left( \\left(a^{[2](i)} - y^{(i)}\\right) w_2^{[2]} \\left(a_2^{[1](i)}\\left(1-a_2^{[1](i)}\\right)\\right)\n",
    "x_1^{(i)}\\right) & \n",
    "\\sum_{i=1}^{m} \\left( \\left(a^{[2](i)} - y^{(i)}\\right) w_2^{[2]} \\left(a_2^{[1](i)}\\left(1-a_2^{[1](i)}\\right)\\right)\n",
    "x_2^{(i)}\\right)\\end{bmatrix}\\tag{12}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at this, you can notice that all terms and indices somehow are very consistent, so it all can be unified into a matrix form. And that's true! $\\left(W^{[2]}\\right)^T = \\begin{bmatrix}w_1^{[2]} \\\\ w_2^{[2]}\\end{bmatrix}$ of size $\\left(n_h \\times n_y\\right) = \\left(2 \\times 1\\right)$ can be multiplied with the vector $A^{[2]} - Y$ of size $\\left(n_y \\times m\\right) = \\left(1 \\times m\\right)$, resulting in a matrix of size $\\left(n_h \\times m\\right) = \\left(2 \\times m\\right)$:\n",
    "\n",
    "$$\\left(W^{[2]}\\right)^T \\left(A^{[2]} - Y\\right)=\n",
    "\\begin{bmatrix}w_1^{[2]} \\\\ w_2^{[2]}\\end{bmatrix}\n",
    "\\begin{bmatrix}\\left(a^{[2](1)} - y^{(1)}\\right) &  \\cdots & \\left(a^{[2](m)} - y^{(m)}\\right)\\end{bmatrix}$$ \n",
    "$$ = \\begin{bmatrix}\n",
    "\\left(a^{[2](1)} - y^{(1)}\\right) w_1^{[2]} & \\cdots & \\left(a^{[2](m)} - y^{(m)}\\right) w_1^{[2]} \\\\\n",
    "\\left(a^{[2](1)} - y^{(1)}\\right) w_2^{[2]} & \\cdots & \\left(a^{[2](m)} - y^{(m)}\\right) w_2^{[2]} \\end{bmatrix}$$\n",
    "\n",
    "Now taking matrix $A^{[1]}$ of the same size $\\left(n_h \\times m\\right) = \\left(2 \\times m\\right)$,\n",
    "\n",
    "$$A^{[1]}\n",
    "=\\begin{bmatrix}\n",
    "a_1^{[1](1)} & \\cdots & a_1^{[1](m)} \\\\\n",
    "a_2^{[1](1)} & \\cdots & a_2^{[1](m)} \\end{bmatrix},$$\n",
    "\n",
    "you can calculate:\n",
    "\n",
    "$$A^{[1]}\\cdot\\left(1-A^{[1]}\\right)\n",
    "=\\begin{bmatrix}\n",
    "a_1^{[1](1)}\\left(1 - a_1^{[1](1)}\\right) & \\cdots & a_1^{[1](m)}\\left(1 - a_1^{[1](m)}\\right) \\\\\n",
    "a_2^{[1](1)}\\left(1 - a_2^{[1](1)}\\right) & \\cdots & a_2^{[1](m)}\\left(1 - a_2^{[1](m)}\\right) \\end{bmatrix},$$\n",
    "\n",
    "where \"$\\cdot$\" denotes **element by element** multiplication.\n",
    "\n",
    "With the element by element multiplication,\n",
    "\n",
    "$$\\left(W^{[2]}\\right)^T \\left(A^{[2]} - Y\\right)\\cdot \\left(A^{[1]}\\cdot\\left(1-A^{[1]}\\right)\\right)=\\begin{bmatrix}\n",
    "\\left(a^{[2](1)} - y^{(1)}\\right) w_1^{[2]}\\left(a_1^{[1](1)}\\left(1 - a_1^{[1](1)}\\right)\\right) & \\cdots & \\left(a^{[2](m)} - y^{(m)}\\right) w_1^{[2]}\\left(a_1^{[1](m)}\\left(1 - a_1^{[1](m)}\\right)\\right) \\\\\n",
    "\\left(a^{[2](1)} - y^{(1)}\\right) w_2^{[2]}\\left(a_2^{[1](1)}\\left(1 - a_2^{[1](1)}\\right)\\right) & \\cdots & \\left(a^{[2](m)} - y^{(m)}\\right) w_2^{[2]} \\left(a_2^{[1](m)}\\left(1 - a_2^{[1](m)}\\right)\\right) \\end{bmatrix}.$$\n",
    "\n",
    "If you perform matrix multiplication with $X^T$ of size $\\left(m \\times n_x\\right) = \\left(m \\times 2\\right)$, you will get matrix of size $\\left(n_h \\times n_x\\right) = \\left(2 \\times 2\\right)$:\n",
    "\n",
    "$$\\left(\\left(W^{[2]}\\right)^T \\left(A^{[2]} - Y\\right)\\cdot \\left(A^{[1]}\\cdot\\left(1-A^{[1]}\\right)\\right)\\right)X^T = \n",
    "\\begin{bmatrix}\n",
    "\\left(a^{[2](1)} - y^{(1)}\\right) w_1^{[2]}\\left(a_1^{[1](1)}\\left(1 - a_1^{[1](1)}\\right)\\right) & \\cdots & \\left(a^{[2](m)} - y^{(m)}\\right) w_1^{[2]}\\left(a_1^{[1](m)}\\left(1 - a_1^{[1](m)}\\right)\\right) \\\\\n",
    "\\left(a^{[2](1)} - y^{(1)}\\right) w_2^{[2]}\\left(a_2^{[1](1)}\\left(1 - a_2^{[1](1)}\\right)\\right) & \\cdots & \\left(a^{[2](m)} - y^{(m)}\\right) w_2^{[2]} \\left(a_2^{[1](m)}\\left(1 - a_2^{[1](m)}\\right)\\right) \\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "x_1^{(1)} & x_2^{(1)} \\\\\n",
    "\\cdots & \\cdots \\\\\n",
    "x_1^{(m)} & x_2^{(m)}\n",
    "\\end{bmatrix}$$\n",
    "$$=\\begin{bmatrix}\n",
    "\\sum_{i=1}^{m} \\left( \\left(a^{[2](i)} - y^{(i)}\\right) w_1^{[2]} \\left(a_1^{[1](i)}\\left(1 - a_1^{[1](i)}\\right) \\right)\n",
    "x_1^{(i)}\\right) & \n",
    "\\sum_{i=1}^{m} \\left( \\left(a^{[2](i)} - y^{(i)}\\right) w_1^{[2]} \\left(a_1^{[1](i)}\\left(1-a_1^{[1](i)}\\right)\\right)\n",
    "x_2^{(i)}\\right)  \\\\\n",
    "\\sum_{i=1}^{m} \\left( \\left(a^{[2](i)} - y^{(i)}\\right) w_2^{[2]} \\left(a_2^{[1](i)}\\left(1-a_2^{[1](i)}\\right)\\right)\n",
    "x_1^{(i)}\\right) & \n",
    "\\sum_{i=1}^{m} \\left( \\left(a^{[2](i)} - y^{(i)}\\right) w_2^{[2]} \\left(a_2^{[1](i)}\\left(1-a_2^{[1](i)}\\right)\\right)\n",
    "x_2^{(i)}\\right)\\end{bmatrix}$$\n",
    "\n",
    "This is exactly like in the expression $(12)$! So, $\\frac{\\partial \\mathcal{L} }{ \\partial W^{[1]}}$ can be written as a mixture of multiplications:\n",
    "\n",
    "$$\\frac{\\partial \\mathcal{L} }{ \\partial W^{[1]}} = \\frac{1}{m}\\left(\\left(W^{[2]}\\right)^T \\left(A^{[2]} - Y\\right)\\cdot \\left(A^{[1]}\\cdot\\left(1-A^{[1]}\\right)\\right)\\right)X^T\\tag{13},$$\n",
    "\n",
    "where \"$\\cdot$\" denotes element by element multiplications.\n",
    "\n",
    "Vector $\\frac{\\partial \\mathcal{L} }{ \\partial b^{[1]}}$ can be found very similarly, but the last terms in the chain rule will be equal to $1$, i.e. $\\frac{\\partial z_1^{[1](i)}}{ \\partial b_1^{[1]}} = 1$. Thus,\n",
    "\n",
    "$$\\frac{\\partial \\mathcal{L} }{ \\partial b^{[1]}} = \\frac{1}{m}\\left(\\left(W^{[2]}\\right)^T \\left(A^{[2]} - Y\\right)\\cdot \\left(A^{[1]}\\cdot\\left(1-A^{[1]}\\right)\\right)\\right)\\mathbf{1},\\tag{14}$$\n",
    "\n",
    "where $\\mathbf{1}$ is a ($m \\times 1$) vector of ones.\n",
    "\n",
    "Expressions $(10)$, $(13)$ and $(14)$ can be used for the parameters update $(9)$ performing backward propagation:\n",
    "\n",
    "\\begin{align}\n",
    "\\frac{\\partial \\mathcal{L} }{ \\partial W^{[2]} } &= \n",
    "\\frac{1}{m}\\left(A^{[2]}-Y\\right)\\left(A^{[1]}\\right)^T,\\\\\n",
    "\\frac{\\partial \\mathcal{L} }{ \\partial b^{[2]} } &= \n",
    "\\frac{1}{m}\\left(A^{[2]}-Y\\right)\\mathbf{1},\\\\\n",
    "\\frac{\\partial \\mathcal{L} }{ \\partial W^{[1]}} &= \\frac{1}{m}\\left(\\left(W^{[2]}\\right)^T \\left(A^{[2]} - Y\\right)\\cdot \\left(A^{[1]}\\cdot\\left(1-A^{[1]}\\right)\\right)\\right)X^T,\\\\\n",
    "\\frac{\\partial \\mathcal{L} }{ \\partial b^{[1]}} &= \\frac{1}{m}\\left(\\left(W^{[2]}\\right)^T \\left(A^{[2]} - Y\\right)\\cdot \\left(A^{[1]}\\cdot\\left(1-A^{[1]}\\right)\\right)\\right)\\mathbf{1},\\\\\n",
    "\\tag{15}\n",
    "\\end{align}\n",
    "\n",
    "where $\\mathbf{1}$ is a ($m \\times 1$) vector of ones.\n",
    "\n",
    "(Taken from Calculus from Machine Learning and Data Science - Coursera)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
